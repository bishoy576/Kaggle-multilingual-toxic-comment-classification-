{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877fb278",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-07T19:42:22.437680Z",
     "iopub.status.busy": "2024-09-07T19:42:22.437363Z",
     "iopub.status.idle": "2024-09-07T19:42:23.481259Z",
     "shell.execute_reply": "2024-09-07T19:42:23.480563Z"
    },
    "papermill": {
     "duration": 1.05216,
     "end_time": "2024-09-07T19:42:23.483168",
     "exception": false,
     "start_time": "2024-09-07T19:42:22.431008",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test_labels.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large/config.json\n",
      "/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large/tf_model.h5\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ca2ec2e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:42:23.492565Z",
     "iopub.status.busy": "2024-09-07T19:42:23.492256Z",
     "iopub.status.idle": "2024-09-07T19:42:38.291561Z",
     "shell.execute_reply": "2024-09-07T19:42:38.290690Z"
    },
    "papermill": {
     "duration": 14.80649,
     "end_time": "2024-09-07T19:42:38.293760",
     "exception": false,
     "start_time": "2024-09-07T19:42:23.487270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19\r\n",
      "  Downloading tokenizers-0.19.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m72.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\r\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\r\n",
      "Collecting regex!=2019.12.17\r\n",
      "  Downloading regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (778 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting safetensors>=0.4.1\r\n",
      "  Downloading safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.1/436.1 kB\u001b[0m \u001b[31m26.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.12.1)\r\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\r\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.6.3)\r\n",
      "Collecting fsspec>=2023.5.0\r\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\r\n",
      "Installing collected packages: safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\r\n",
      "Successfully installed fsspec-2024.9.0 huggingface-hub-0.24.6 regex-2024.7.24 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abcc13f9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:42:38.305810Z",
     "iopub.status.busy": "2024-09-07T19:42:38.305361Z",
     "iopub.status.idle": "2024-09-07T19:42:42.889032Z",
     "shell.execute_reply": "2024-09-07T19:42:42.888001Z"
    },
    "papermill": {
     "duration": 4.592507,
     "end_time": "2024-09-07T19:42:42.891536",
     "exception": false,
     "start_time": "2024-09-07T19:42:38.299029",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\r\n",
      "Successfully installed sentencepiece-0.2.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b03d3f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:42:42.904058Z",
     "iopub.status.busy": "2024-09-07T19:42:42.903790Z",
     "iopub.status.idle": "2024-09-07T19:43:38.525334Z",
     "shell.execute_reply": "2024-09-07T19:43:38.524558Z"
    },
    "papermill": {
     "duration": 55.630533,
     "end_time": "2024-09-07T19:43:38.527577",
     "exception": false,
     "start_time": "2024-09-07T19:42:42.897044",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D0907 19:43:14.959937278      14 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\n",
      "D0907 19:43:14.959963400      14 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\n",
      "D0907 19:43:14.959967013      14 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\n",
      "D0907 19:43:14.959969930      14 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\n",
      "D0907 19:43:14.959972431      14 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\n",
      "D0907 19:43:14.959975468      14 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\n",
      "D0907 19:43:14.959977967      14 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\n",
      "D0907 19:43:14.959980174      14 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\n",
      "D0907 19:43:14.959982371      14 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\n",
      "D0907 19:43:14.959984555      14 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\n",
      "D0907 19:43:14.959986810      14 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\n",
      "D0907 19:43:14.959989073      14 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\n",
      "D0907 19:43:14.959991771      14 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\n",
      "D0907 19:43:14.959993984      14 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\n",
      "I0907 19:43:14.960208356      14 ev_epoll1_linux.cc:122]               grpc epoll fd: 59\n",
      "D0907 19:43:14.960220749      14 ev_posix.cc:144]                      Using polling engine: epoll1\n",
      "D0907 19:43:14.960240912      14 dns_resolver_ares.cc:822]             Using ares dns resolver\n",
      "D0907 19:43:14.960739745      14 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\n",
      "D0907 19:43:14.960748879      14 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\n",
      "D0907 19:43:14.960752767      14 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\n",
      "D0907 19:43:14.960756090      14 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\n",
      "D0907 19:43:14.960759932      14 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\n",
      "D0907 19:43:14.960763388      14 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\n",
      "D0907 19:43:14.960771946      14 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\n",
      "D0907 19:43:14.960789287      14 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\n",
      "D0907 19:43:14.960826538      14 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\n",
      "D0907 19:43:14.960849078      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\n",
      "D0907 19:43:14.960852995      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D0907 19:43:14.960862348      14 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\n",
      "D0907 19:43:14.960866189      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D0907 19:43:14.960870036      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\n",
      "D0907 19:43:14.960873907      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\n",
      "D0907 19:43:14.960878925      14 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\n",
      "I0907 19:43:14.964328448      14 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\n",
      "I0907 19:43:14.999512446      14 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\n",
      "E0907 19:43:15.005515857      14 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {grpc_status:2, created_time:\"2024-09-07T19:43:15.005500497+00:00\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 192 \n",
    "DROPOUT = 0.5 # use aggressive dropout\n",
    "BATCH_SIZE = 16 # per TPU core\n",
    "TOTAL_STEPS_STAGE1 = 2000\n",
    "VALIDATE_EVERY_STAGE1 = 200\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "### Different learning rate for transformer and head ###\n",
    "LR_TRANSFORMER = 5e-6\n",
    "LR_HEAD = 1e-3\n",
    "\n",
    "PRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n",
    "PRETRAINED_MODEL = '/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large'\n",
    "D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "D_TRANS = '/kaggle/input/jigsaw-train-multilingual-coments-google-api/'\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from transformers import TFRobertaModel, AutoTokenizer\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4689e31b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:43:38.540257Z",
     "iopub.status.busy": "2024-09-07T19:43:38.539818Z",
     "iopub.status.idle": "2024-09-07T19:43:38.543603Z",
     "shell.execute_reply": "2024-09-07T19:43:38.542851Z"
    },
    "papermill": {
     "duration": 0.012135,
     "end_time": "2024-09-07T19:43:38.545306",
     "exception": false,
     "start_time": "2024-09-07T19:43:38.533171",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fed106a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:43:38.557438Z",
     "iopub.status.busy": "2024-09-07T19:43:38.557203Z",
     "iopub.status.idle": "2024-09-07T19:43:47.719729Z",
     "shell.execute_reply": "2024-09-07T19:43:47.718482Z"
    },
    "papermill": {
     "duration": 9.170885,
     "end_time": "2024-09-07T19:43:47.721422",
     "exception": false,
     "start_time": "2024-09-07T19:43:38.550537",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  \n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "91a670f7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:43:47.736208Z",
     "iopub.status.busy": "2024-09-07T19:43:47.735955Z",
     "iopub.status.idle": "2024-09-07T19:44:07.707028Z",
     "shell.execute_reply": "2024-09-07T19:44:07.705855Z"
    },
    "papermill": {
     "duration": 19.980814,
     "end_time": "2024-09-07T19:44:07.709282",
     "exception": false,
     "start_time": "2024-09-07T19:43:47.728468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n",
    "                      columns=['comment_text', 'toxic']):\n",
    "    train_6langs=[]\n",
    "    for i in range(len(langs)):\n",
    "        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s-cleaned.csv'%langs[i]\n",
    "        train_6langs.append(downsample(pd.read_csv(fn)[columns]))\n",
    "\n",
    "    return train_6langs\n",
    "\n",
    "def downsample(df):\n",
    "    \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n",
    "    ds_df= pd.concat([\n",
    "        df.query('toxic==1'),\n",
    "        df.query('toxic==0').sample(sum(df.toxic))\n",
    "    ])\n",
    "    \n",
    "    return ds_df\n",
    "    \n",
    "\n",
    "train_df = pd.concat(load_jigsaw_trans()) \n",
    "val_df = pd.read_csv(D+'validation.csv')\n",
    "test_df = pd.read_csv(D+'test.csv')\n",
    "sub_df = pd.read_csv(D+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e96d5621",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:44:07.723566Z",
     "iopub.status.busy": "2024-09-07T19:44:07.723269Z",
     "iopub.status.idle": "2024-09-07T19:44:50.314638Z",
     "shell.execute_reply": "2024-09-07T19:44:50.313744Z"
    },
    "papermill": {
     "duration": 42.608768,
     "end_time": "2024-09-07T19:44:50.324705",
     "exception": false,
     "start_time": "2024-09-07T19:44:07.715937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 9min 58s, sys: 23.6 s, total: 10min 22s\n",
      "Wall time: 42.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "X_train = regular_encode(train_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.toxic.values.reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "24db4be3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:44:50.340499Z",
     "iopub.status.busy": "2024-09-07T19:44:50.340177Z",
     "iopub.status.idle": "2024-09-07T19:44:51.292924Z",
     "shell.execute_reply": "2024-09-07T19:44:51.291879Z"
    },
    "papermill": {
     "duration": 0.963216,
     "end_time": "2024-09-07T19:44:51.295282",
     "exception": false,
     "start_time": "2024-09-07T19:44:50.332066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "val_dist_dataset   = create_dist_dataset(X_val)\n",
    "test_dist_dataset  = create_dist_dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "85bfa76a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:44:51.309933Z",
     "iopub.status.busy": "2024-09-07T19:44:51.309597Z",
     "iopub.status.idle": "2024-09-07T19:45:49.466778Z",
     "shell.execute_reply": "2024-09-07T19:45:49.465864Z"
    },
    "papermill": {
     "duration": 58.166567,
     "end_time": "2024-09-07T19:45:49.468625",
     "exception": false,
     "start_time": "2024-09-07T19:44:51.302058",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some layers from the model checkpoint at /kaggle/input/jigsaw-mlm-finetuned-xlm-r-large were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the layers of TFRobertaModel were initialized from the model checkpoint at /kaggle/input/jigsaw-mlm-finetuned-xlm-r-large.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaModel for predictions without further training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_word_ids (InputLayer)  [(None, 192)]            0         \n",
      "                                                                 \n",
      " tf_roberta_model (TFRoberta  TFBaseModelOutputWithPoo  559890432\n",
      " Model)                      lingAndCrossAttentions(l            \n",
      "                             ast_hidden_state=(None,             \n",
      "                             192, 1024),                         \n",
      "                              pooler_output=(None, 10            \n",
      "                             24),                                \n",
      "                              past_key_values=None, h            \n",
      "                             idden_states=None, atten            \n",
      "                             tions=None, cross_attent            \n",
      "                             ions=None)                          \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 1024)             0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " custom_head (Dense)         (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 42.2 s, sys: 46 s, total: 1min 28s\n",
      "Wall time: 58.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL)  \n",
    "#         transformer_layer = TransformerLayer(base_transformer)\n",
    "        model = build_model(transformer_layer)\n",
    "        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n",
    "        optimizer_head = Adam(learning_rate=LR_HEAD)\n",
    "    return model, optimizer_transformer, optimizer_head\n",
    "\n",
    "\n",
    "def build_model(transformer):\n",
    "    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one\n",
    "    # let's slice out the first position, the paper says its not worse than pooling\n",
    "    \n",
    "    x = transformer(inp)[0][:, 0, :]  \n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    ### note, adding the name to later identify these weights for different LR\n",
    "    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31111376",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:45:49.487420Z",
     "iopub.status.busy": "2024-09-07T19:45:49.486508Z",
     "iopub.status.idle": "2024-09-07T19:45:52.738776Z",
     "shell.execute_reply": "2024-09-07T19:45:52.737657Z"
    },
    "papermill": {
     "duration": 3.263539,
     "end_time": "2024-09-07T19:45:52.740604",
     "exception": false,
     "start_time": "2024-09-07T19:45:49.477065",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "23f49c88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:45:52.760283Z",
     "iopub.status.busy": "2024-09-07T19:45:52.759508Z",
     "iopub.status.idle": "2024-09-07T19:45:55.083401Z",
     "shell.execute_reply": "2024-09-07T19:45:55.082088Z"
    },
    "papermill": {
     "duration": 2.33632,
     "end_time": "2024-09-07T19:45:55.085797",
     "exception": false,
     "start_time": "2024-09-07T19:45:52.749477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbishoymeseha\u001b[0m (\u001b[33mbishoymeseha-n-a\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240907_194552-25jt0cbk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bishoymeseha-n-a/roberta/runs/25jt0cbk\" target=\"_blank\">1</a></strong> to <a href=\"https://wandb.ai/bishoymeseha-n-a/roberta\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n",
    "\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        # AUC and Loss Metrics\n",
    "        train_auc_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "        val_auc_metric = tf.keras.metrics.AUC(name='validation_AUC')\n",
    "        train_loss_metric = tf.keras.metrics.Mean(name='training_loss')  # Tracking loss\n",
    "        val_loss_metric = tf.keras.metrics.Mean(name='validation_loss')  # Validation loss\n",
    "\n",
    "    return compute_loss, train_auc_metric, val_auc_metric, train_loss_metric, val_loss_metric\n",
    "\n",
    "# Initialize wandb before training\n",
    "wandb.init(project=\"roberta\", name=\"1\") # Set project name and experiment name\n",
    "\n",
    "def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n",
    "          total_steps=2000, validate_every=200):\n",
    "    best_weights, history = None, []\n",
    "    step = 0\n",
    "\n",
    "    ### Training loop ###\n",
    "    for tensor in train_dist_dataset:\n",
    "        distributed_train_step(tensor)\n",
    "        step += 1\n",
    "\n",
    "        if (step % validate_every == 0):   \n",
    "            ### Train Metrics ###\n",
    "            train_auc = train_auc_metric.result().numpy()\n",
    "            train_loss = train_loss_metric.result().numpy()\n",
    "\n",
    "            print(\"Step %d, train AUC: %.5f, train Loss: %.5f\" % (step, train_auc, train_loss))\n",
    "\n",
    "            # Wandb log for training\n",
    "            wandb.log({\"train_AUC\": train_auc, \"train_loss\": train_loss, \"step\": step})\n",
    "\n",
    "            ### Validation Metrics ###\n",
    "            if val_dist_dataset:\n",
    "                val_predictions = predict(val_dist_dataset)\n",
    "                val_auc = roc_auc_score(y_val, val_predictions)\n",
    "                val_loss = compute_val_loss(y_val, val_predictions)  # Custom validation loss computation\n",
    "\n",
    "                print(\"Step %d, val AUC: %.5f, val Loss: %.5f\" % (step, val_auc, val_loss))\n",
    "\n",
    "                # Wandb log for validation\n",
    "                wandb.log({\"val_AUC\": val_auc, \"val_loss\": val_loss, \"step\": step})\n",
    "\n",
    "                history.append(val_auc)\n",
    "                if history[-1] == max(history):\n",
    "                    best_weights = model.get_weights()\n",
    "\n",
    "            ### Reset metrics ###\n",
    "            train_auc_metric.reset_states()\n",
    "            train_loss_metric.reset_states()\n",
    "\n",
    "        if step == total_steps:\n",
    "            break\n",
    "\n",
    "    ### Restore best weights ###\n",
    "    model.set_weights(best_weights)\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data):\n",
    "    strategy.run(train_step, args=(data,))\n",
    "\n",
    "def train_step(inputs):\n",
    "    features, labels = inputs\n",
    "\n",
    "    ### Separate transformer and head variables\n",
    "    transformer_trainable_variables = [v for v in model.trainable_variables \n",
    "                                       if (('pooler' not in v.name)  and ('custom' not in v.name))]\n",
    "    head_trainable_variables = [v for v in model.trainable_variables \n",
    "                                if 'custom' in v.name]\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "\n",
    "    gradients_transformer = tape.gradient(loss, transformer_trainable_variables)\n",
    "    gradients_head = tape.gradient(loss, head_trainable_variables)\n",
    "    del tape\n",
    "\n",
    "    optimizer_transformer.apply_gradients(zip(gradients_transformer, transformer_trainable_variables))\n",
    "    optimizer_head.apply_gradients(zip(gradients_head, head_trainable_variables))\n",
    "\n",
    "    train_auc_metric.update_state(labels, predictions)\n",
    "    train_loss_metric.update_state(loss)  # Log training loss metric\n",
    "\n",
    "def predict(dataset):  \n",
    "    predictions = []\n",
    "    for tensor in dataset:\n",
    "        predictions.append(distributed_prediction_step(tensor))\n",
    "    predictions = np.vstack(list(map(np.vstack, predictions)))\n",
    "    return predictions\n",
    "\n",
    "@tf.function\n",
    "def distributed_prediction_step(data):\n",
    "    predictions = strategy.run(prediction_step, args=(data,))\n",
    "    return strategy.experimental_local_results(predictions)\n",
    "\n",
    "def prediction_step(inputs):\n",
    "    features = inputs\n",
    "    predictions = model(features, training=False)\n",
    "    return predictions\n",
    "\n",
    "def compute_val_loss(y_val, val_predictions):\n",
    "    # Custom function to calculate validation loss\n",
    "    val_loss = tf.keras.losses.binary_crossentropy(y_val, val_predictions, from_logits=False)\n",
    "    return tf.reduce_mean(val_loss).numpy()\n",
    "\n",
    "# Initialize metrics\n",
    "compute_loss, train_auc_metric, val_auc_metric, train_loss_metric, val_loss_metric = define_losses_and_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1be23ca6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T19:45:55.106128Z",
     "iopub.status.busy": "2024-09-07T19:45:55.105841Z",
     "iopub.status.idle": "2024-09-07T20:01:11.845758Z",
     "shell.execute_reply": "2024-09-07T20:01:11.844735Z"
    },
    "papermill": {
     "duration": 916.758973,
     "end_time": "2024-09-07T20:01:11.854408",
     "exception": false,
     "start_time": "2024-09-07T19:45:55.095435",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:`TPUStrategy.run` is called with [args: ((PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(16, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(16, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(16, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(16, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(16, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(16, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(16, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(16, 192) dtype=int64>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor 'data_8:0' shape=(16, 1) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_9:0' shape=(16, 1) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_10:0' shape=(16, 1) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_11:0' shape=(16, 1) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_12:0' shape=(16, 1) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_13:0' shape=(16, 1) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_14:0' shape=(16, 1) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_15:0' shape=(16, 1) dtype=int64>\n",
      "}),)] [kwargs: None]\n",
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:459: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "DEBUG:absl:`TPUStrategy.run` is called with [args: ((PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(16, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(16, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(16, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(16, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(16, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(16, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(16, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(16, 192) dtype=int64>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor 'data_8:0' shape=(16, 1) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_9:0' shape=(16, 1) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_10:0' shape=(16, 1) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_11:0' shape=(16, 1) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_12:0' shape=(16, 1) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_13:0' shape=(16, 1) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_14:0' shape=(16, 1) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_15:0' shape=(16, 1) dtype=int64>\n",
      "}),)] [kwargs: None]\n",
      "2024-09-07 19:46:59.846728: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2024-09-07 19:47:01.576983: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, train AUC: 0.88737, train Loss: 0.05321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:`TPUStrategy.run` is called with [args: (PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(16, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(16, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(16, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(16, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(16, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(16, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(16, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(16, 192) dtype=int64>\n",
      "},)] [kwargs: None]\n",
      "2024-09-07 19:49:29.180414: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "2024-09-07 19:49:29.418187: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "DEBUG:absl:`TPUStrategy.run` is called with [args: (PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(8, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(8, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(8, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(8, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(8, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(8, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(8, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(8, 192) dtype=int64>\n",
      "},)] [kwargs: None]\n",
      "2024-09-07 19:49:47.305128: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "2024-09-07 19:49:47.540428: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, val AUC: 0.94196, val Loss: 0.31791\n",
      "Step 400, train AUC: 0.96723, train Loss: 0.02861\n",
      "Step 400, val AUC: 0.94416, val Loss: 0.35647\n",
      "Step 600, train AUC: 0.96795, train Loss: 0.02810\n",
      "Step 600, val AUC: 0.94265, val Loss: 0.36436\n",
      "Step 800, train AUC: 0.97120, train Loss: 0.02667\n",
      "Step 800, val AUC: 0.94390, val Loss: 0.33151\n",
      "Step 1000, train AUC: 0.97227, train Loss: 0.02614\n",
      "Step 1000, val AUC: 0.94408, val Loss: 0.36111\n",
      "Step 1200, train AUC: 0.97377, train Loss: 0.02533\n",
      "Step 1200, val AUC: 0.94245, val Loss: 0.30119\n",
      "Step 1400, train AUC: 0.97413, train Loss: 0.02510\n",
      "Step 1400, val AUC: 0.94591, val Loss: 0.29165\n",
      "Step 1600, train AUC: 0.97434, train Loss: 0.02488\n",
      "Step 1600, val AUC: 0.94476, val Loss: 0.36254\n",
      "Step 1800, train AUC: 0.97619, train Loss: 0.02391\n",
      "Step 1800, val AUC: 0.94656, val Loss: 0.22958\n",
      "Step 2000, train AUC: 0.97630, train Loss: 0.02399\n",
      "Step 2000, val AUC: 0.94270, val Loss: 0.34940\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>train_AUC</td><td>▁▇▇███████</td></tr><tr><td>train_loss</td><td>█▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_AUC</td><td>▁▄▂▄▄▂▇▅█▂</td></tr><tr><td>val_loss</td><td>▆██▆█▅▄█▁▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>2000</td></tr><tr><td>train_AUC</td><td>0.9763</td></tr><tr><td>train_loss</td><td>0.02399</td></tr><tr><td>val_AUC</td><td>0.9427</td></tr><tr><td>val_loss</td><td>0.3494</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">1</strong>: <a href=\"https://wandb.ai/bishoymeseha-n-a/roberta/runs/25jt0cbk\" target=\"_blank\">https://wandb.ai/bishoymeseha-n-a/roberta/runs/25jt0cbk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240907_194552-25jt0cbk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 30s, sys: 1min 55s, total: 15min 26s\n",
      "Wall time: 15min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd178008",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T20:01:11.879661Z",
     "iopub.status.busy": "2024-09-07T20:01:11.879126Z",
     "iopub.status.idle": "2024-09-07T20:01:47.150106Z",
     "shell.execute_reply": "2024-09-07T20:01:47.148924Z"
    },
    "papermill": {
     "duration": 35.286127,
     "end_time": "2024-09-07T20:01:47.152454",
     "exception": false,
     "start_time": "2024-09-07T20:01:11.866327",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train = regular_encode(train_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.toxic.values.reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c6979943",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T20:01:47.176557Z",
     "iopub.status.busy": "2024-09-07T20:01:47.176290Z",
     "iopub.status.idle": "2024-09-07T20:04:18.323141Z",
     "shell.execute_reply": "2024-09-07T20:04:18.322039Z"
    },
    "papermill": {
     "duration": 151.161285,
     "end_time": "2024-09-07T20:04:18.325174",
     "exception": false,
     "start_time": "2024-09-07T20:01:47.163889",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240907_200147-2cdmidvk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bishoymeseha-n-a/roberta/runs/2cdmidvk\" target=\"_blank\">2</a></strong> to <a href=\"https://wandb.ai/bishoymeseha-n-a/roberta\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, train AUC: 0.94353, train Loss: 0.02634\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:`TPUStrategy.run` is called with [args: (PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(4, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(4, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(4, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(4, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(4, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(4, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(4, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(4, 192) dtype=int64>\n",
      "},)] [kwargs: None]\n",
      "2024-09-07 20:01:58.075096: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "2024-09-07 20:01:58.287329: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, val AUC: 0.95789, val Loss: 0.17753\n",
      "Step 20, train AUC: 0.94678, train Loss: 0.02439\n",
      "Step 20, val AUC: 0.95823, val Loss: 0.17526\n",
      "Step 30, train AUC: 0.92387, train Loss: 0.03073\n",
      "Step 30, val AUC: 0.95889, val Loss: 0.18570\n",
      "Step 40, train AUC: 0.94235, train Loss: 0.02772\n",
      "Step 40, val AUC: 0.95770, val Loss: 0.17246\n",
      "Step 50, train AUC: 0.95052, train Loss: 0.02336\n",
      "Step 50, val AUC: 0.95857, val Loss: 0.17269\n",
      "Step 60, train AUC: 0.95655, train Loss: 0.02139\n",
      "Step 60, val AUC: 0.96025, val Loss: 0.17141\n",
      "Step 70, train AUC: 0.95515, train Loss: 0.02322\n",
      "Step 70, val AUC: 0.96247, val Loss: 0.17117\n",
      "Step 80, train AUC: 0.95911, train Loss: 0.02197\n",
      "Step 80, val AUC: 0.96281, val Loss: 0.16800\n",
      "Step 90, train AUC: 0.95898, train Loss: 0.02330\n",
      "Step 90, val AUC: 0.96370, val Loss: 0.16713\n",
      "Step 100, train AUC: 0.95872, train Loss: 0.02212\n",
      "Step 100, val AUC: 0.96408, val Loss: 0.16608\n",
      "Step 110, train AUC: 0.96040, train Loss: 0.02157\n",
      "Step 110, val AUC: 0.96414, val Loss: 0.17549\n",
      "Step 120, train AUC: 0.96622, train Loss: 0.01958\n",
      "Step 120, val AUC: 0.96216, val Loss: 0.17173\n",
      "Step 130, train AUC: 0.97302, train Loss: 0.01849\n",
      "Step 130, val AUC: 0.96219, val Loss: 0.17098\n",
      "Step 140, train AUC: 0.97835, train Loss: 0.01711\n",
      "Step 140, val AUC: 0.96403, val Loss: 0.20684\n",
      "Step 150, train AUC: 0.96770, train Loss: 0.01924\n",
      "Step 150, val AUC: 0.96174, val Loss: 0.17744\n",
      "Step 160, train AUC: 0.96381, train Loss: 0.02232\n",
      "Step 160, val AUC: 0.96335, val Loss: 0.16970\n",
      "Step 170, train AUC: 0.96093, train Loss: 0.02184\n",
      "Step 170, val AUC: 0.96403, val Loss: 0.16939\n",
      "Step 180, train AUC: 0.98326, train Loss: 0.01518\n",
      "Step 180, val AUC: 0.95934, val Loss: 0.19138\n",
      "Step 190, train AUC: 0.97526, train Loss: 0.01846\n",
      "Step 190, val AUC: 0.96301, val Loss: 0.18374\n",
      "Step 200, train AUC: 0.97402, train Loss: 0.01824\n",
      "Step 200, val AUC: 0.96206, val Loss: 0.18209\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_AUC</td><td>▃▄▁▃▄▅▅▅▅▅▅▆▇▇▆▆▅█▇▇</td></tr><tr><td>train_loss</td><td>▆▅█▇▅▄▅▄▅▄▄▃▂▂▃▄▄▁▂▂</td></tr><tr><td>val_AUC</td><td>▁▂▂▁▂▄▆▇███▆▆█▅▇█▃▇▆</td></tr><tr><td>val_loss</td><td>▃▃▄▂▂▂▂▁▁▁▃▂▂█▃▂▂▅▄▄</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>200</td></tr><tr><td>train_AUC</td><td>0.97402</td></tr><tr><td>train_loss</td><td>0.01824</td></tr><tr><td>val_AUC</td><td>0.96206</td></tr><tr><td>val_loss</td><td>0.18209</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">2</strong>: <a href=\"https://wandb.ai/bishoymeseha-n-a/roberta/runs/2cdmidvk\" target=\"_blank\">https://wandb.ai/bishoymeseha-n-a/roberta/runs/2cdmidvk</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240907_200147-2cdmidvk/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "%pdb on\n",
    "\n",
    "# decrease LR for second stage in the head\n",
    "optimizer_head.learning_rate.assign(1e-4)\n",
    "\n",
    "# split validation data into train test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_val, y_val, test_size = 0.1)\n",
    "\n",
    "# make a datasets\n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, training=True)\n",
    "val_dist_dataset = create_dist_dataset(X_val)\n",
    "wandb.init(project=\"roberta\", name=\"2\")# train again\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      total_steps = TOTAL_STEPS_STAGE2, \n",
    "      validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e78db2f8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T20:04:18.355409Z",
     "iopub.status.busy": "2024-09-07T20:04:18.355125Z",
     "iopub.status.idle": "2024-09-07T20:05:33.283452Z",
     "shell.execute_reply": "2024-09-07T20:05:33.282513Z"
    },
    "papermill": {
     "duration": 74.945773,
     "end_time": "2024-09-07T20:05:33.285312",
     "exception": false,
     "start_time": "2024-09-07T20:04:18.339539",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:`TPUStrategy.run` is called with [args: (PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(9, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(9, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(9, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(9, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(9, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(9, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(9, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(5, 192) dtype=int64>\n",
      "},)] [kwargs: None]\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, (<tf.Tensor 'data:0' shape=(9, 192) dtype=int64>,), {}]\n",
      "2024-09-07 20:05:19.723509: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "2024-09-07 20:05:20.046608: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 8.71 s, total: 1min 18s\n",
      "Wall time: 1min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub_df['toxic'] = predict(test_dist_dataset)[:,0]\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b94cb0d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-07T20:05:33.317142Z",
     "iopub.status.busy": "2024-09-07T20:05:33.316825Z",
     "iopub.status.idle": "2024-09-07T20:05:33.360995Z",
     "shell.execute_reply": "2024-09-07T20:05:33.360169Z"
    },
    "papermill": {
     "duration": 0.061925,
     "end_time": "2024-09-07T20:05:33.362687",
     "exception": false,
     "start_time": "2024-09-07T20:05:33.300762",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9414304246027457\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "ytrue = pd.read_csv(D+'test_labels.csv')['toxic']\n",
    "print(roc_auc_score(ytrue, sub_df['toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a903d8",
   "metadata": {
    "papermill": {
     "duration": 0.014229,
     "end_time": "2024-09-07T20:05:33.391058",
     "exception": false,
     "start_time": "2024-09-07T20:05:33.376829",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 2703900,
     "sourceId": 19018,
     "sourceType": "competition"
    },
    {
     "datasetId": 588377,
     "sourceId": 1062669,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 697854,
     "sourceId": 1220647,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30514,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1400.921397,
   "end_time": "2024-09-07T20:05:41.285938",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-07T19:42:20.364541",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
