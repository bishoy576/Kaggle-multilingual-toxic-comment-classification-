{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c4ecb311",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-09-10T19:46:50.407494Z",
     "iopub.status.busy": "2024-09-10T19:46:50.407120Z",
     "iopub.status.idle": "2024-09-10T19:46:51.532804Z",
     "shell.execute_reply": "2024-09-10T19:46:51.532068Z"
    },
    "papermill": {
     "duration": 1.134155,
     "end_time": "2024-09-10T19:46:51.534781",
     "exception": false,
     "start_time": "2024-09-10T19:46:50.400626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/mlm-on-training-data/config.json\n",
      "/kaggle/input/mlm-on-training-data/__results__.html\n",
      "/kaggle/input/mlm-on-training-data/tf_model.h5\n",
      "/kaggle/input/mlm-on-training-data/__notebook__.ipynb\n",
      "/kaggle/input/mlm-on-training-data/__output__.json\n",
      "/kaggle/input/mlm-on-training-data/custom.css\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-pt-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-it-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-ru-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es-cleaned.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-fr.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-es.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr.csv\n",
      "/kaggle/input/jigsaw-train-multilingual-coments-google-api/jigsaw-toxic-comment-train-google-tr-cleaned.csv\n",
      "/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large/config.json\n",
      "/kaggle/input/jigsaw-mlm-finetuned-xlm-r-large/tf_model.h5\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/sample_submission.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test_labels.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train-processed-seqlen128.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/validation.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/test.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-unintended-bias-train.csv\n",
      "/kaggle/input/jigsaw-multilingual-toxic-comment-classification/jigsaw-toxic-comment-train-processed-seqlen128.csv\n"
     ]
    }
   ],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2892e9b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:46:51.544843Z",
     "iopub.status.busy": "2024-09-10T19:46:51.544494Z",
     "iopub.status.idle": "2024-09-10T19:47:06.842547Z",
     "shell.execute_reply": "2024-09-10T19:47:06.841410Z"
    },
    "papermill": {
     "duration": 15.30549,
     "end_time": "2024-09-10T19:47:06.844912",
     "exception": false,
     "start_time": "2024-09-10T19:46:51.539422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\r\n",
      "  Downloading transformers-4.44.2-py3-none-any.whl (9.5 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/site-packages (from transformers) (6.0)\r\n",
      "Collecting huggingface-hub<1.0,>=0.23.2\r\n",
      "  Downloading huggingface_hub-0.24.6-py3-none-any.whl (417 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.5/417.5 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting tokenizers<0.20,>=0.19\r\n",
      "  Downloading tokenizers-0.19.1-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m67.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/site-packages (from transformers) (4.65.0)\r\n",
      "Collecting safetensors>=0.4.1\r\n",
      "  Downloading safetensors-0.4.5-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (436 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m436.1/436.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hCollecting regex!=2019.12.17\r\n",
      "  Downloading regex-2024.7.24-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (778 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m778.9/778.9 kB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/site-packages (from transformers) (3.12.1)\r\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/site-packages (from transformers) (23.1)\r\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.8/site-packages (from transformers) (2.31.0)\r\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/site-packages (from transformers) (1.23.5)\r\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.6.3)\r\n",
      "Collecting fsspec>=2023.5.0\r\n",
      "  Downloading fsspec-2024.9.0-py3-none-any.whl (179 kB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (2023.5.7)\r\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (3.1.0)\r\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.8/site-packages (from requests->transformers) (1.26.16)\r\n",
      "Installing collected packages: safetensors, regex, fsspec, huggingface-hub, tokenizers, transformers\r\n",
      "Successfully installed fsspec-2024.9.0 huggingface-hub-0.24.6 regex-2024.7.24 safetensors-0.4.5 tokenizers-0.19.1 transformers-4.44.2\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4397a188",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:47:06.857174Z",
     "iopub.status.busy": "2024-09-10T19:47:06.856881Z",
     "iopub.status.idle": "2024-09-10T19:47:11.542116Z",
     "shell.execute_reply": "2024-09-10T19:47:11.540998Z"
    },
    "papermill": {
     "duration": 4.693723,
     "end_time": "2024-09-10T19:47:11.544204",
     "exception": false,
     "start_time": "2024-09-10T19:47:06.850481",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sentencepiece\r\n",
      "  Downloading sentencepiece-0.2.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: sentencepiece\r\n",
      "Successfully installed sentencepiece-0.2.0\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\r\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\r\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8e4ca47",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:47:11.556977Z",
     "iopub.status.busy": "2024-09-10T19:47:11.556697Z",
     "iopub.status.idle": "2024-09-10T19:48:07.999021Z",
     "shell.execute_reply": "2024-09-10T19:48:07.998230Z"
    },
    "papermill": {
     "duration": 56.451535,
     "end_time": "2024-09-10T19:48:08.001276",
     "exception": false,
     "start_time": "2024-09-10T19:47:11.549741",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D0910 19:47:43.755465326      14 config.cc:119]                        gRPC EXPERIMENT tcp_frame_size_tuning               OFF (default:OFF)\n",
      "D0910 19:47:43.755490088      14 config.cc:119]                        gRPC EXPERIMENT tcp_rcv_lowat                       OFF (default:OFF)\n",
      "D0910 19:47:43.755493735      14 config.cc:119]                        gRPC EXPERIMENT peer_state_based_framing            OFF (default:OFF)\n",
      "D0910 19:47:43.755496338      14 config.cc:119]                        gRPC EXPERIMENT flow_control_fixes                  ON  (default:ON)\n",
      "D0910 19:47:43.755498795      14 config.cc:119]                        gRPC EXPERIMENT memory_pressure_controller          OFF (default:OFF)\n",
      "D0910 19:47:43.755501249      14 config.cc:119]                        gRPC EXPERIMENT unconstrained_max_quota_buffer_size OFF (default:OFF)\n",
      "D0910 19:47:43.755505725      14 config.cc:119]                        gRPC EXPERIMENT new_hpack_huffman_decoder           ON  (default:ON)\n",
      "D0910 19:47:43.755507978      14 config.cc:119]                        gRPC EXPERIMENT event_engine_client                 OFF (default:OFF)\n",
      "D0910 19:47:43.755510356      14 config.cc:119]                        gRPC EXPERIMENT monitoring_experiment               ON  (default:ON)\n",
      "D0910 19:47:43.755512863      14 config.cc:119]                        gRPC EXPERIMENT promise_based_client_call           OFF (default:OFF)\n",
      "D0910 19:47:43.755515193      14 config.cc:119]                        gRPC EXPERIMENT free_large_allocator                OFF (default:OFF)\n",
      "D0910 19:47:43.755517518      14 config.cc:119]                        gRPC EXPERIMENT promise_based_server_call           OFF (default:OFF)\n",
      "D0910 19:47:43.755519887      14 config.cc:119]                        gRPC EXPERIMENT transport_supplies_client_latency   OFF (default:OFF)\n",
      "D0910 19:47:43.755522240      14 config.cc:119]                        gRPC EXPERIMENT event_engine_listener               OFF (default:OFF)\n",
      "I0910 19:47:43.755776147      14 ev_epoll1_linux.cc:122]               grpc epoll fd: 59\n",
      "D0910 19:47:43.755841994      14 ev_posix.cc:144]                      Using polling engine: epoll1\n",
      "D0910 19:47:43.755865907      14 dns_resolver_ares.cc:822]             Using ares dns resolver\n",
      "D0910 19:47:43.756468218      14 lb_policy_registry.cc:46]             registering LB policy factory for \"priority_experimental\"\n",
      "D0910 19:47:43.756476300      14 lb_policy_registry.cc:46]             registering LB policy factory for \"outlier_detection_experimental\"\n",
      "D0910 19:47:43.756479646      14 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_target_experimental\"\n",
      "D0910 19:47:43.756482562      14 lb_policy_registry.cc:46]             registering LB policy factory for \"pick_first\"\n",
      "D0910 19:47:43.756485715      14 lb_policy_registry.cc:46]             registering LB policy factory for \"round_robin\"\n",
      "D0910 19:47:43.756488601      14 lb_policy_registry.cc:46]             registering LB policy factory for \"weighted_round_robin_experimental\"\n",
      "D0910 19:47:43.756496366      14 lb_policy_registry.cc:46]             registering LB policy factory for \"ring_hash_experimental\"\n",
      "D0910 19:47:43.756516350      14 lb_policy_registry.cc:46]             registering LB policy factory for \"grpclb\"\n",
      "D0910 19:47:43.756549561      14 lb_policy_registry.cc:46]             registering LB policy factory for \"rls_experimental\"\n",
      "D0910 19:47:43.756569831      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_manager_experimental\"\n",
      "D0910 19:47:43.756573906      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_impl_experimental\"\n",
      "D0910 19:47:43.756577347      14 lb_policy_registry.cc:46]             registering LB policy factory for \"cds_experimental\"\n",
      "D0910 19:47:43.756580730      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_cluster_resolver_experimental\"\n",
      "D0910 19:47:43.756584058      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_override_host_experimental\"\n",
      "D0910 19:47:43.756587115      14 lb_policy_registry.cc:46]             registering LB policy factory for \"xds_wrr_locality_experimental\"\n",
      "D0910 19:47:43.756591393      14 certificate_provider_registry.cc:35]  registering certificate provider factory for \"file_watcher\"\n",
      "I0910 19:47:43.760451609      14 socket_utils_common_posix.cc:408]     Disabling AF_INET6 sockets because ::1 is not available.\n",
      "I0910 19:47:43.781900698      14 socket_utils_common_posix.cc:337]     TCP_USER_TIMEOUT is available. TCP_USER_TIMEOUT will be used thereafter\n",
      "E0910 19:47:43.787936954      14 oauth2_credentials.cc:236]            oauth_fetch: UNKNOWN:C-ares status is not ARES_SUCCESS qtype=A name=metadata.google.internal. is_balancer=0: Domain name not found {created_time:\"2024-09-10T19:47:43.787920262+00:00\", grpc_status:2}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.12.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "MAX_LEN = 192 \n",
    "DROPOUT = 0.5 # use aggressive dropout\n",
    "BATCH_SIZE = 16 # per TPU core\n",
    "TOTAL_STEPS_STAGE1 = 2000\n",
    "VALIDATE_EVERY_STAGE1 = 200\n",
    "TOTAL_STEPS_STAGE2 = 200\n",
    "VALIDATE_EVERY_STAGE2 = 10\n",
    "\n",
    "### Different learning rate for transformer and head ###\n",
    "LR_TRANSFORMER = 5e-6\n",
    "LR_HEAD = 1e-3\n",
    "\n",
    "PRETRAINED_TOKENIZER=  'jplu/tf-xlm-roberta-large'\n",
    "PRETRAINED_MODEL = '/kaggle/input/mlm-on-training-data'\n",
    "D = '/kaggle/input/jigsaw-multilingual-toxic-comment-classification/'\n",
    "D_TRANS = '/kaggle/input/jigsaw-train-multilingual-coments-google-api/'\n",
    "\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sentencepiece\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "from tensorflow.keras.layers import Dense, Input, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import transformers\n",
    "from transformers import TFRobertaModel, AutoTokenizer\n",
    "import logging\n",
    "# no extensive logging \n",
    "logging.getLogger().setLevel(logging.NOTSET)\n",
    "\n",
    "AUTO = tf.data.experimental.AUTOTUNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5aa77501",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:48:08.014602Z",
     "iopub.status.busy": "2024-09-10T19:48:08.014091Z",
     "iopub.status.idle": "2024-09-10T19:48:08.017942Z",
     "shell.execute_reply": "2024-09-10T19:48:08.017301Z"
    },
    "papermill": {
     "duration": 0.01211,
     "end_time": "2024-09-10T19:48:08.019510",
     "exception": false,
     "start_time": "2024-09-10T19:48:08.007400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "02e322c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:48:08.032212Z",
     "iopub.status.busy": "2024-09-10T19:48:08.031805Z",
     "iopub.status.idle": "2024-09-10T19:48:16.432783Z",
     "shell.execute_reply": "2024-09-10T19:48:16.431967Z"
    },
    "papermill": {
     "duration": 8.410864,
     "end_time": "2024-09-10T19:48:16.436192",
     "exception": false,
     "start_time": "2024-09-10T19:48:08.025328",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on TPU  \n",
      "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n",
      "INFO:tensorflow:Initializing the TPU system: local\n",
      "INFO:tensorflow:Finished initializing TPU system.\n",
      "INFO:tensorflow:Found TPU system:\n",
      "INFO:tensorflow:*** Num TPU Cores: 8\n",
      "INFO:tensorflow:*** Num TPU Workers: 1\n",
      "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n",
      "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n",
      "REPLICAS:  8\n"
     ]
    }
   ],
   "source": [
    "def connect_to_TPU():\n",
    "    \"\"\"Detect hardware, return appropriate distribution strategy\"\"\"\n",
    "    try:\n",
    "        # TPU detection. No parameters necessary if TPU_NAME environment variable is\n",
    "        # set: this is always the case on Kaggle.\n",
    "        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "        print('Running on TPU ', tpu.master())\n",
    "    except ValueError:\n",
    "        tpu = None\n",
    "\n",
    "    if tpu:\n",
    "        tf.config.experimental_connect_to_cluster(tpu)\n",
    "        tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "        strategy = tf.distribute.TPUStrategy(tpu)\n",
    "    else:\n",
    "        # Default distribution strategy in Tensorflow. Works on CPU and single GPU.\n",
    "        strategy = tf.distribute.get_strategy()\n",
    "\n",
    "    global_batch_size = BATCH_SIZE * strategy.num_replicas_in_sync\n",
    "\n",
    "    return tpu, strategy, global_batch_size\n",
    "\n",
    "\n",
    "tpu, strategy, global_batch_size = connect_to_TPU()\n",
    "print(\"REPLICAS: \", strategy.num_replicas_in_sync)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a2ec05d1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:48:16.451395Z",
     "iopub.status.busy": "2024-09-10T19:48:16.451123Z",
     "iopub.status.idle": "2024-09-10T19:48:36.014627Z",
     "shell.execute_reply": "2024-09-10T19:48:36.013354Z"
    },
    "papermill": {
     "duration": 19.574124,
     "end_time": "2024-09-10T19:48:36.017340",
     "exception": false,
     "start_time": "2024-09-10T19:48:16.443216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_jigsaw_trans(langs=['tr','it','es','ru','fr','pt'], \n",
    "                      columns=['comment_text', 'toxic']):\n",
    "    train_6langs=[]\n",
    "    for i in range(len(langs)):\n",
    "        fn = D_TRANS+'jigsaw-toxic-comment-train-google-%s-cleaned.csv'%langs[i]\n",
    "        train_6langs.append(downsample(pd.read_csv(fn)[columns]))\n",
    "\n",
    "    return train_6langs\n",
    "\n",
    "def downsample(df):\n",
    "    \"\"\"Subsample the train dataframe to 50%-50%\"\"\"\n",
    "    ds_df= pd.concat([\n",
    "        df.query('toxic==1'),\n",
    "        df.query('toxic==0').sample(sum(df.toxic))\n",
    "    ])\n",
    "    \n",
    "    return ds_df\n",
    "    \n",
    "\n",
    "train_df = pd.concat(load_jigsaw_trans()) \n",
    "val_df = pd.read_csv(D+'validation.csv')\n",
    "test_df = pd.read_csv(D+'test.csv')\n",
    "sub_df = pd.read_csv(D+'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6d851fa4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:48:36.032257Z",
     "iopub.status.busy": "2024-09-10T19:48:36.031955Z",
     "iopub.status.idle": "2024-09-10T19:49:21.769347Z",
     "shell.execute_reply": "2024-09-10T19:49:21.768425Z"
    },
    "papermill": {
     "duration": 45.753726,
     "end_time": "2024-09-10T19:49:21.777857",
     "exception": false,
     "start_time": "2024-09-10T19:48:36.024131",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 14min 35s, sys: 25.3 s, total: 15min 1s\n",
      "Wall time: 45.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "def regular_encode(texts, tokenizer, maxlen=512):\n",
    "    enc_di = tokenizer.batch_encode_plus(\n",
    "        texts, \n",
    "        return_token_type_ids=False,\n",
    "        pad_to_max_length=True,\n",
    "        max_length=maxlen\n",
    "    )\n",
    "    \n",
    "    return np.array(enc_di['input_ids'])\n",
    "    \n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(PRETRAINED_TOKENIZER)\n",
    "X_train = regular_encode(train_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.toxic.values.reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14c15c71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:49:21.793052Z",
     "iopub.status.busy": "2024-09-10T19:49:21.792778Z",
     "iopub.status.idle": "2024-09-10T19:49:22.750493Z",
     "shell.execute_reply": "2024-09-10T19:49:22.749504Z"
    },
    "papermill": {
     "duration": 0.968176,
     "end_time": "2024-09-10T19:49:22.752890",
     "exception": false,
     "start_time": "2024-09-10T19:49:21.784714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_dist_dataset(X, y=None, training=False):\n",
    "    dataset = tf.data.Dataset.from_tensor_slices(X)\n",
    "\n",
    "    ### Add y if present ###\n",
    "    if y is not None:\n",
    "        dataset_y = tf.data.Dataset.from_tensor_slices(y)\n",
    "        dataset = tf.data.Dataset.zip((dataset, dataset_y))\n",
    "        \n",
    "    ### Repeat if training ###\n",
    "    if training:\n",
    "        dataset = dataset.shuffle(len(X)).repeat()\n",
    "\n",
    "    dataset = dataset.batch(global_batch_size).prefetch(AUTO)\n",
    "\n",
    "    ### make it distributed  ###\n",
    "    dist_dataset = strategy.experimental_distribute_dataset(dataset)\n",
    "\n",
    "    return dist_dataset\n",
    "    \n",
    "    \n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, True)\n",
    "val_dist_dataset   = create_dist_dataset(X_val)\n",
    "test_dist_dataset  = create_dist_dataset(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a91dea42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:49:22.768118Z",
     "iopub.status.busy": "2024-09-10T19:49:22.767844Z",
     "iopub.status.idle": "2024-09-10T19:50:18.753984Z",
     "shell.execute_reply": "2024-09-10T19:50:18.752558Z"
    },
    "papermill": {
     "duration": 55.996158,
     "end_time": "2024-09-10T19:50:18.756191",
     "exception": false,
     "start_time": "2024-09-10T19:49:22.760033",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using a model of type xlm-roberta to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n",
      "Some layers from the model checkpoint at /kaggle/input/mlm-on-training-data were not used when initializing TFRobertaModel: ['lm_head']\n",
      "- This IS expected if you are initializing TFRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFRobertaModel were not initialized from the model checkpoint at /kaggle/input/mlm-on-training-data and are newly initialized: ['roberta/pooler/dense/bias:0', 'roberta/pooler/dense/kernel:0']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_word_ids (InputLayer)  [(None, 192)]            0         \n",
      "                                                                 \n",
      " tf_roberta_model (TFRoberta  TFBaseModelOutputWithPoo  559890432\n",
      " Model)                      lingAndCrossAttentions(l            \n",
      "                             ast_hidden_state=(None,             \n",
      "                             192, 1024),                         \n",
      "                              pooler_output=(None, 10            \n",
      "                             24),                                \n",
      "                              past_key_values=None, h            \n",
      "                             idden_states=None, atten            \n",
      "                             tions=None, cross_attent            \n",
      "                             ions=None)                          \n",
      "                                                                 \n",
      " tf.__operators__.getitem (S  (None, 1024)             0         \n",
      " licingOpLambda)                                                 \n",
      "                                                                 \n",
      " dropout_73 (Dropout)        (None, 1024)              0         \n",
      "                                                                 \n",
      " custom_head (Dense)         (None, 1)                 1025      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 559,891,457\n",
      "Trainable params: 559,891,457\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "CPU times: user 41.3 s, sys: 46 s, total: 1min 27s\n",
      "Wall time: 56 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "def create_model_and_optimizer():\n",
    "    with strategy.scope():\n",
    "        transformer_layer = TFRobertaModel.from_pretrained(PRETRAINED_MODEL)  \n",
    "#         transformer_layer = TransformerLayer(base_transformer)\n",
    "        model = build_model(transformer_layer)\n",
    "        optimizer_transformer = Adam(learning_rate=LR_TRANSFORMER)\n",
    "        optimizer_head = Adam(learning_rate=LR_HEAD)\n",
    "    return model, optimizer_transformer, optimizer_head\n",
    "\n",
    "\n",
    "def build_model(transformer):\n",
    "    inp = Input(shape=(MAX_LEN,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    # Huggingface transformers have multiple outputs, embeddings are the first one\n",
    "    # let's slice out the first position, the paper says its not worse than pooling\n",
    "    \n",
    "    x = transformer(inp)[0][:, 0, :]  \n",
    "    x = Dropout(DROPOUT)(x)\n",
    "    ### note, adding the name to later identify these weights for different LR\n",
    "    out = Dense(1, activation='sigmoid', name='custom_head')(x)\n",
    "    model = Model(inputs=[inp], outputs=[out])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "model, optimizer_transformer, optimizer_head = create_model_and_optimizer()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "941ffeb2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:50:18.776492Z",
     "iopub.status.busy": "2024-09-10T19:50:18.776196Z",
     "iopub.status.idle": "2024-09-10T19:50:22.157373Z",
     "shell.execute_reply": "2024-09-10T19:50:22.156545Z"
    },
    "papermill": {
     "duration": 3.393874,
     "end_time": "2024-09-10T19:50:22.159437",
     "exception": false,
     "start_time": "2024-09-10T19:50:18.765563",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983d916e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:50:22.179161Z",
     "iopub.status.busy": "2024-09-10T19:50:22.178893Z",
     "iopub.status.idle": "2024-09-10T19:50:24.782151Z",
     "shell.execute_reply": "2024-09-10T19:50:24.780857Z"
    },
    "papermill": {
     "duration": 2.616259,
     "end_time": "2024-09-10T19:50:24.785022",
     "exception": false,
     "start_time": "2024-09-10T19:50:22.168763",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbishoymeseha\u001b[0m (\u001b[33mbishoymeseha-n-a\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240910_195022-tnnd9atc</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bishoymeseha-n-a/roberta/runs/tnnd9atc\" target=\"_blank\">1</a></strong> to <a href=\"https://wandb.ai/bishoymeseha-n-a/roberta\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def define_losses_and_metrics():\n",
    "    with strategy.scope():\n",
    "        loss_object = tf.keras.losses.BinaryCrossentropy(\n",
    "            reduction=tf.keras.losses.Reduction.NONE, from_logits=False)\n",
    "\n",
    "        def compute_loss(labels, predictions):\n",
    "            per_example_loss = loss_object(labels, predictions)\n",
    "            loss = tf.nn.compute_average_loss(\n",
    "                per_example_loss, global_batch_size = global_batch_size)\n",
    "            return loss\n",
    "\n",
    "        # AUC and Loss Metrics\n",
    "        train_auc_metric = tf.keras.metrics.AUC(name='training_AUC')\n",
    "        val_auc_metric = tf.keras.metrics.AUC(name='validation_AUC')\n",
    "        train_loss_metric = tf.keras.metrics.Mean(name='training_loss')  # Tracking loss\n",
    "        val_loss_metric = tf.keras.metrics.Mean(name='validation_loss')  # Validation loss\n",
    "\n",
    "    return compute_loss, train_auc_metric, val_auc_metric, train_loss_metric, val_loss_metric\n",
    "\n",
    "# Initialize wandb before training\n",
    "wandb.init(project=\"roberta\", name=\"1\") # Set project name and experiment name\n",
    "\n",
    "def train(train_dist_dataset, val_dist_dataset=None, y_val=None,\n",
    "          total_steps=2000, validate_every=200):\n",
    "    best_weights, history = None, []\n",
    "    step = 0\n",
    "\n",
    "    ### Training loop ###\n",
    "    for tensor in train_dist_dataset:\n",
    "        distributed_train_step(tensor)\n",
    "        step += 1\n",
    "\n",
    "        if (step % validate_every == 0):   \n",
    "            ### Train Metrics ###\n",
    "            train_auc = train_auc_metric.result().numpy()\n",
    "            train_loss = train_loss_metric.result().numpy()\n",
    "\n",
    "            print(\"Step %d, train AUC: %.5f, train Loss: %.5f\" % (step, train_auc, train_loss))\n",
    "\n",
    "            # Wandb log for training\n",
    "            wandb.log({\"train_AUC\": train_auc, \"train_loss\": train_loss, \"step\": step})\n",
    "\n",
    "            ### Validation Metrics ###\n",
    "            if val_dist_dataset:\n",
    "                val_predictions = predict(val_dist_dataset)\n",
    "                val_auc = roc_auc_score(y_val, val_predictions)\n",
    "                val_loss = compute_val_loss(y_val, val_predictions)  # Custom validation loss computation\n",
    "\n",
    "                print(\"Step %d, val AUC: %.5f, val Loss: %.5f\" % (step, val_auc, val_loss))\n",
    "\n",
    "                # Wandb log for validation\n",
    "                wandb.log({\"val_AUC\": val_auc, \"val_loss\": val_loss, \"step\": step})\n",
    "\n",
    "                history.append(val_auc)\n",
    "                if history[-1] == max(history):\n",
    "                    best_weights = model.get_weights()\n",
    "\n",
    "            ### Reset metrics ###\n",
    "            train_auc_metric.reset_states()\n",
    "            train_loss_metric.reset_states()\n",
    "\n",
    "        if step == total_steps:\n",
    "            break\n",
    "\n",
    "    ### Restore best weights ###\n",
    "    model.set_weights(best_weights)\n",
    "\n",
    "@tf.function\n",
    "def distributed_train_step(data):\n",
    "    strategy.run(train_step, args=(data,))\n",
    "\n",
    "def train_step(inputs):\n",
    "    features, labels = inputs\n",
    "\n",
    "    ### Separate transformer and head variables\n",
    "    transformer_trainable_variables = [v for v in model.trainable_variables \n",
    "                                       if (('pooler' not in v.name)  and ('custom' not in v.name))]\n",
    "    head_trainable_variables = [v for v in model.trainable_variables \n",
    "                                if 'custom' in v.name]\n",
    "\n",
    "    with tf.GradientTape(persistent=True) as tape:\n",
    "        predictions = model(features, training=True)\n",
    "        loss = compute_loss(labels, predictions)\n",
    "\n",
    "    gradients_transformer = tape.gradient(loss, transformer_trainable_variables)\n",
    "    gradients_head = tape.gradient(loss, head_trainable_variables)\n",
    "    del tape\n",
    "\n",
    "    optimizer_transformer.apply_gradients(zip(gradients_transformer, transformer_trainable_variables))\n",
    "    optimizer_head.apply_gradients(zip(gradients_head, head_trainable_variables))\n",
    "\n",
    "    train_auc_metric.update_state(labels, predictions)\n",
    "    train_loss_metric.update_state(loss)  # Log training loss metric\n",
    "\n",
    "def predict(dataset):  \n",
    "    predictions = []\n",
    "    for tensor in dataset:\n",
    "        predictions.append(distributed_prediction_step(tensor))\n",
    "    predictions = np.vstack(list(map(np.vstack, predictions)))\n",
    "    return predictions\n",
    "\n",
    "@tf.function\n",
    "def distributed_prediction_step(data):\n",
    "    predictions = strategy.run(prediction_step, args=(data,))\n",
    "    return strategy.experimental_local_results(predictions)\n",
    "\n",
    "def prediction_step(inputs):\n",
    "    features = inputs\n",
    "    predictions = model(features, training=False)\n",
    "    return predictions\n",
    "\n",
    "def compute_val_loss(y_val, val_predictions):\n",
    "    # Custom function to calculate validation loss\n",
    "    val_loss = tf.keras.losses.binary_crossentropy(y_val, val_predictions, from_logits=False)\n",
    "    return tf.reduce_mean(val_loss).numpy()\n",
    "\n",
    "# Initialize metrics\n",
    "compute_loss, train_auc_metric, val_auc_metric, train_loss_metric, val_loss_metric = define_losses_and_metrics()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3574c0a2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T19:50:24.808554Z",
     "iopub.status.busy": "2024-09-10T19:50:24.808250Z",
     "iopub.status.idle": "2024-09-10T20:05:39.514057Z",
     "shell.execute_reply": "2024-09-10T20:05:39.513457Z"
    },
    "papermill": {
     "duration": 914.719878,
     "end_time": "2024-09-10T20:05:39.515826",
     "exception": false,
     "start_time": "2024-09-10T19:50:24.795948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:`TPUStrategy.run` is called with [args: ((PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(16, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(16, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(16, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(16, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(16, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(16, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(16, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(16, 192) dtype=int64>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor 'data_8:0' shape=(16, 1) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_9:0' shape=(16, 1) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_10:0' shape=(16, 1) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_11:0' shape=(16, 1) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_12:0' shape=(16, 1) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_13:0' shape=(16, 1) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_14:0' shape=(16, 1) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_15:0' shape=(16, 1) dtype=int64>\n",
      "}),)] [kwargs: None]\n",
      "/usr/local/lib/python3.8/site-packages/tensorflow/python/framework/indexed_slices.py:459: UserWarning: Converting sparse IndexedSlices to a dense Tensor with 256002048 elements. This may consume a large amount of memory.\n",
      "  warnings.warn(\n",
      "DEBUG:absl:`TPUStrategy.run` is called with [args: ((PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(16, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(16, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(16, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(16, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(16, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(16, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(16, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(16, 192) dtype=int64>\n",
      "}, PerReplica:{\n",
      "  0: <tf.Tensor 'data_8:0' shape=(16, 1) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_9:0' shape=(16, 1) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_10:0' shape=(16, 1) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_11:0' shape=(16, 1) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_12:0' shape=(16, 1) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_13:0' shape=(16, 1) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_14:0' shape=(16, 1) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_15:0' shape=(16, 1) dtype=int64>\n",
      "}),)] [kwargs: None]\n",
      "2024-09-10 19:51:28.613579: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n",
      "2024-09-10 19:51:30.145323: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node AssignAddVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, train AUC: 0.89893, train Loss: 0.05052\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:`TPUStrategy.run` is called with [args: (PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(16, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(16, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(16, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(16, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(16, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(16, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(16, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(16, 192) dtype=int64>\n",
      "},)] [kwargs: None]\n",
      "2024-09-10 19:53:56.982716: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "2024-09-10 19:53:57.213105: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "DEBUG:absl:`TPUStrategy.run` is called with [args: (PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(8, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(8, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(8, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(8, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(8, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(8, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(8, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(8, 192) dtype=int64>\n",
      "},)] [kwargs: None]\n",
      "2024-09-10 19:54:14.985145: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "2024-09-10 19:54:15.196017: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 200, val AUC: 0.94030, val Loss: 0.31926\n",
      "Step 400, train AUC: 0.96399, train Loss: 0.02990\n",
      "Step 400, val AUC: 0.94254, val Loss: 0.34037\n",
      "Step 600, train AUC: 0.96964, train Loss: 0.02738\n",
      "Step 600, val AUC: 0.94301, val Loss: 0.31733\n",
      "Step 800, train AUC: 0.97171, train Loss: 0.02638\n",
      "Step 800, val AUC: 0.94619, val Loss: 0.37626\n",
      "Step 1000, train AUC: 0.96985, train Loss: 0.02728\n",
      "Step 1000, val AUC: 0.94507, val Loss: 0.28994\n",
      "Step 1200, train AUC: 0.97490, train Loss: 0.02478\n",
      "Step 1200, val AUC: 0.94391, val Loss: 0.38870\n",
      "Step 1400, train AUC: 0.97452, train Loss: 0.02493\n",
      "Step 1400, val AUC: 0.94113, val Loss: 0.27095\n",
      "Step 1600, train AUC: 0.97457, train Loss: 0.02475\n",
      "Step 1600, val AUC: 0.93846, val Loss: 0.38209\n",
      "Step 1800, train AUC: 0.97457, train Loss: 0.02498\n",
      "Step 1800, val AUC: 0.93491, val Loss: 0.41446\n",
      "Step 2000, train AUC: 0.97581, train Loss: 0.02418\n",
      "Step 2000, val AUC: 0.94235, val Loss: 0.29155\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▂▂▃▃▃▃▄▄▅▅▆▆▆▆▇▇██</td></tr><tr><td>train_AUC</td><td>▁▇▇█▇█████</td></tr><tr><td>train_loss</td><td>█▃▂▂▂▁▁▁▁▁</td></tr><tr><td>val_AUC</td><td>▄▆▆█▇▇▅▃▁▆</td></tr><tr><td>val_loss</td><td>▃▄▃▆▂▇▁▆█▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>2000</td></tr><tr><td>train_AUC</td><td>0.97581</td></tr><tr><td>train_loss</td><td>0.02418</td></tr><tr><td>val_AUC</td><td>0.94235</td></tr><tr><td>val_loss</td><td>0.29155</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">1</strong>: <a href=\"https://wandb.ai/bishoymeseha-n-a/roberta/runs/tnnd9atc\" target=\"_blank\">https://wandb.ai/bishoymeseha-n-a/roberta/runs/tnnd9atc</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240910_195022-tnnd9atc/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 13min 32s, sys: 1min 50s, total: 15min 23s\n",
      "Wall time: 15min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      TOTAL_STEPS_STAGE1, VALIDATE_EVERY_STAGE1)\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e92a5f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T20:05:39.540672Z",
     "iopub.status.busy": "2024-09-10T20:05:39.540129Z",
     "iopub.status.idle": "2024-09-10T20:06:16.389421Z",
     "shell.execute_reply": "2024-09-10T20:06:16.388105Z"
    },
    "papermill": {
     "duration": 36.864438,
     "end_time": "2024-09-10T20:06:16.392010",
     "exception": false,
     "start_time": "2024-09-10T20:05:39.527572",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/transformers/tokenization_utils_base.py:2870: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train = regular_encode(train_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "X_val = regular_encode(val_df.comment_text.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "X_test = regular_encode(test_df.content.values.tolist(), tokenizer, maxlen=MAX_LEN)\n",
    "\n",
    "y_train = train_df.toxic.values.reshape(-1,1)\n",
    "y_val = val_df.toxic.values.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "88e93256",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T20:06:16.416834Z",
     "iopub.status.busy": "2024-09-10T20:06:16.416566Z",
     "iopub.status.idle": "2024-09-10T20:08:47.378380Z",
     "shell.execute_reply": "2024-09-10T20:08:47.377259Z"
    },
    "papermill": {
     "duration": 150.976804,
     "end_time": "2024-09-10T20:08:47.380840",
     "exception": false,
     "start_time": "2024-09-10T20:06:16.404036",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic pdb calling has been turned ON\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.9 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/kaggle/working/wandb/run-20240910_200616-3do53m6q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/bishoymeseha-n-a/roberta/runs/3do53m6q\" target=\"_blank\">2</a></strong> to <a href=\"https://wandb.ai/bishoymeseha-n-a/roberta\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, train AUC: 0.90704, train Loss: 0.03325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:`TPUStrategy.run` is called with [args: (PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(4, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(4, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(4, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(4, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(4, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(4, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(4, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(4, 192) dtype=int64>\n",
      "},)] [kwargs: None]\n",
      "2024-09-10 20:06:27.327227: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "2024-09-10 20:06:27.543635: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10, val AUC: 0.95272, val Loss: 0.19759\n",
      "Step 20, train AUC: 0.95610, train Loss: 0.02564\n",
      "Step 20, val AUC: 0.95326, val Loss: 0.19308\n",
      "Step 30, train AUC: 0.94472, train Loss: 0.02577\n",
      "Step 30, val AUC: 0.95532, val Loss: 0.19888\n",
      "Step 40, train AUC: 0.94038, train Loss: 0.02756\n",
      "Step 40, val AUC: 0.95768, val Loss: 0.18881\n",
      "Step 50, train AUC: 0.93465, train Loss: 0.02652\n",
      "Step 50, val AUC: 0.95723, val Loss: 0.18779\n",
      "Step 60, train AUC: 0.94548, train Loss: 0.02448\n",
      "Step 60, val AUC: 0.95843, val Loss: 0.18322\n",
      "Step 70, train AUC: 0.95745, train Loss: 0.02222\n",
      "Step 70, val AUC: 0.95997, val Loss: 0.18221\n",
      "Step 80, train AUC: 0.96194, train Loss: 0.02143\n",
      "Step 80, val AUC: 0.95896, val Loss: 0.19739\n",
      "Step 90, train AUC: 0.96269, train Loss: 0.02075\n",
      "Step 90, val AUC: 0.95961, val Loss: 0.18340\n",
      "Step 100, train AUC: 0.94855, train Loss: 0.02565\n",
      "Step 100, val AUC: 0.95921, val Loss: 0.18468\n",
      "Step 110, train AUC: 0.95109, train Loss: 0.02335\n",
      "Step 110, val AUC: 0.95986, val Loss: 0.18734\n",
      "Step 120, train AUC: 0.95393, train Loss: 0.02345\n",
      "Step 120, val AUC: 0.96164, val Loss: 0.17582\n",
      "Step 130, train AUC: 0.95932, train Loss: 0.02249\n",
      "Step 130, val AUC: 0.96108, val Loss: 0.19263\n",
      "Step 140, train AUC: 0.96075, train Loss: 0.02114\n",
      "Step 140, val AUC: 0.96255, val Loss: 0.17286\n",
      "Step 150, train AUC: 0.96763, train Loss: 0.02131\n",
      "Step 150, val AUC: 0.96288, val Loss: 0.17270\n",
      "Step 160, train AUC: 0.96397, train Loss: 0.02022\n",
      "Step 160, val AUC: 0.96257, val Loss: 0.19028\n",
      "Step 170, train AUC: 0.97033, train Loss: 0.01940\n",
      "Step 170, val AUC: 0.96326, val Loss: 0.17745\n",
      "Step 180, train AUC: 0.96757, train Loss: 0.01881\n",
      "Step 180, val AUC: 0.96300, val Loss: 0.17800\n",
      "Step 190, train AUC: 0.97934, train Loss: 0.01628\n",
      "Step 190, val AUC: 0.96296, val Loss: 0.19791\n",
      "Step 200, train AUC: 0.97733, train Loss: 0.01693\n",
      "Step 200, val AUC: 0.96277, val Loss: 0.19149\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▇▇▇▇▇▇████</td></tr><tr><td>train_AUC</td><td>▁▆▅▄▄▅▆▆▆▅▅▆▆▆▇▇▇▇██</td></tr><tr><td>train_loss</td><td>█▅▅▆▅▄▃▃▃▅▄▄▄▃▃▃▂▂▁▁</td></tr><tr><td>val_AUC</td><td>▁▁▃▄▄▅▆▅▆▅▆▇▇███████</td></tr><tr><td>val_loss</td><td>█▆█▅▅▄▄█▄▄▅▂▆▁▁▆▂▂█▆</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>step</td><td>200</td></tr><tr><td>train_AUC</td><td>0.97733</td></tr><tr><td>train_loss</td><td>0.01693</td></tr><tr><td>val_AUC</td><td>0.96277</td></tr><tr><td>val_loss</td><td>0.19149</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">2</strong>: <a href=\"https://wandb.ai/bishoymeseha-n-a/roberta/runs/3do53m6q\" target=\"_blank\">https://wandb.ai/bishoymeseha-n-a/roberta/runs/3do53m6q</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240910_200616-3do53m6q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %%time\n",
    "%pdb on\n",
    "\n",
    "# decrease LR for second stage in the head\n",
    "optimizer_head.learning_rate.assign(1e-4)\n",
    "\n",
    "# split validation data into train test\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_val, y_val, test_size = 0.1)\n",
    "\n",
    "# make a datasets\n",
    "train_dist_dataset = create_dist_dataset(X_train, y_train, training=True)\n",
    "val_dist_dataset = create_dist_dataset(X_val)\n",
    "wandb.init(project=\"roberta\", name=\"2\")# train again\n",
    "train(train_dist_dataset, val_dist_dataset, y_val,\n",
    "      total_steps = TOTAL_STEPS_STAGE2, \n",
    "      validate_every = VALIDATE_EVERY_STAGE2)  # not validating but printing now\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a2a31d3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T20:08:47.415340Z",
     "iopub.status.busy": "2024-09-10T20:08:47.414934Z",
     "iopub.status.idle": "2024-09-10T20:10:03.091839Z",
     "shell.execute_reply": "2024-09-10T20:10:03.090767Z"
    },
    "papermill": {
     "duration": 75.69653,
     "end_time": "2024-09-10T20:10:03.093793",
     "exception": false,
     "start_time": "2024-09-10T20:08:47.397263",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG:absl:`TPUStrategy.run` is called with [args: (PerReplica:{\n",
      "  0: <tf.Tensor 'data:0' shape=(9, 192) dtype=int64>,\n",
      "  1: <tf.Tensor 'data_1:0' shape=(9, 192) dtype=int64>,\n",
      "  2: <tf.Tensor 'data_2:0' shape=(9, 192) dtype=int64>,\n",
      "  3: <tf.Tensor 'data_3:0' shape=(9, 192) dtype=int64>,\n",
      "  4: <tf.Tensor 'data_4:0' shape=(9, 192) dtype=int64>,\n",
      "  5: <tf.Tensor 'data_5:0' shape=(9, 192) dtype=int64>,\n",
      "  6: <tf.Tensor 'data_6:0' shape=(9, 192) dtype=int64>,\n",
      "  7: <tf.Tensor 'data_7:0' shape=(5, 192) dtype=int64>\n",
      "},)] [kwargs: None]\n",
      "INFO:absl:TPU has inputs with dynamic shapes: [<tf.Tensor 'Const:0' shape=() dtype=int32>, (<tf.Tensor 'data:0' shape=(9, 192) dtype=int64>,), {}]\n",
      "2024-09-10 20:09:49.874386: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n",
      "2024-09-10 20:09:50.224708: E tensorflow/core/grappler/optimizers/meta_optimizer.cc:954] model_pruner failed: INVALID_ARGUMENT: Graph does not contain terminal node model/custom_head/BiasAdd/ReadVariableOp.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 9s, sys: 8.73 s, total: 1min 18s\n",
      "Wall time: 1min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sub_df['toxic'] = predict(test_dist_dataset)[:,0]\n",
    "sub_df.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1d195804",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-09-10T20:10:03.125807Z",
     "iopub.status.busy": "2024-09-10T20:10:03.125513Z",
     "iopub.status.idle": "2024-09-10T20:10:03.172290Z",
     "shell.execute_reply": "2024-09-10T20:10:03.171403Z"
    },
    "papermill": {
     "duration": 0.065079,
     "end_time": "2024-09-10T20:10:03.174081",
     "exception": false,
     "start_time": "2024-09-10T20:10:03.109002",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9397015171682328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "ytrue = pd.read_csv(D+'test_labels.csv')['toxic']\n",
    "print(roc_auc_score(ytrue, sub_df['toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2258e8",
   "metadata": {
    "papermill": {
     "duration": 0.015443,
     "end_time": "2024-09-10T20:10:03.205443",
     "exception": false,
     "start_time": "2024-09-10T20:10:03.190000",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 2703900,
     "sourceId": 19018,
     "sourceType": "competition"
    },
    {
     "datasetId": 588377,
     "sourceId": 1062669,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 697854,
     "sourceId": 1220647,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 196109435,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 30514,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1403.088939,
   "end_time": "2024-09-10T20:10:11.368227",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2024-09-10T19:46:48.279288",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
